
# ğŸ§ Retinal Disease Classification with Sequential Expert Training

**Multi-label classification of retinal diseases using a boosting-inspired ensemble of expert models.**

This repository presents a novel training pipeline for classifying retinal diseases from fundus images. Inspired by boosting, it iteratively trains a sequence of expert modelsâ€”each one learning to improve upon the weaknesses of its predecessor. The result is a highly accurate and robust ensemble system for real-world diagnosis tasks.

---

## âœ¨ Highlights

- **ğŸ“š Boosting-Inspired Expert Chain**  
  Each expert focuses on what the previous model got wrongâ€”enabling iterative refinement.

- **âš–ï¸ Dynamic Class Weighting**  
  Automatically recalibrates the loss function to target difficult or rare disease classes.

- **ğŸ§  Model Agnostic**  
  Plug-and-play support for powerful pre-trained architectures:
  - Vision Transformers: ViT, CLIP-ViT, DINO-ViT  
  - ResNeXt  
  - EfficientNet

- **ğŸ§ª Reproducibility First**  
  Built with PyTorch Lightning to enforce modularity and clarity.

- **ğŸ“Š Integrated Experiment Tracking**  
  Every training run is logged in real-time using Weights & Biases.

---

## ğŸ” How It Works: Sequential Expert Training

```mermaid
graph TD
  A[Expert #1: Train Normally] --> B[Evaluate per-class AP]
  B --> C[Reweight: 1 - AP]
  C --> D[Expert #2: Focused Training]
  D --> E[Repeat for N Experts]
```

Each expert is trained to focus on what the previous one missed by dynamically adjusting class weights:

> **Weight Formula**: `class_weight = 1 - average_precision_score`

---

## ğŸ“† Project Structure

```bash
.
â”œâ”€â”€ train.py               # Main training script
â”œâ”€â”€ nets.py                # Modular model definitions
â”œâ”€â”€ data.py                # Dataset loaders (extendable)
â”œâ”€â”€ utils/                 # Helper functions, weights, metrics
â”œâ”€â”€ models/                # Saved checkpoints
â””â”€â”€ README.md
```

---

## ğŸ’ª Installation

```bash
git clone https://github.com/es15326/Multi-Disease-Detection.git
cd Multi-Disease-Detection
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

<details>
<summary><code>requirements.txt</code></summary>

```
torch
torchvision
pytorch-lightning
pandas
numpy
scikit-learn
scikit-image
opencv-python-headless
wandb
transformers
Pillow
```

</details>

---

## ğŸ“ Dataset Format

Organize your data like this:

```
dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ image_001.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â””â”€â”€ image_050.jpg
â”œâ”€â”€ train_labels.csv
â””â”€â”€ val_labels.csv
```

The CSV files should contain one-hot encoded labels:

```csv
image_id,no_finding,disease_1,disease_2,disease_3,...
image_001,0,1,0,1,...
image_002,1,0,0,0,...
```

---

## ğŸš€ Training

```bash
python train.py \
  --train_img_path ./dataset/train \
  --val_img_path ./dataset/val \
  --train_csv ./dataset/train_labels.csv \
  --val_csv ./dataset/val_labels.csv \
  --arch vit-b \
  --resolution 512 \
  --batch_size 16 \
  --max_epochs 100 \
  --boosting_experts 10 \
  --devices 0 1 \
  --savedir ./models \
  --tag vit-b-run-1
```

### ğŸ”§ Notable Flags

| Flag | Description |
|------|-------------|
| `--arch` | Model architecture (e.g., `vit-b`, `resnext`, `efficientnet`) |
| `--boosting_experts` | Number of sequential experts |
| `--freeze-backbone` | Optional: Only train classifier head |
| `--savedir` | Checkpoint and weight output directory |

---

## ğŸ“ˆ Monitoring & Results

- **ğŸ” Weights & Biases**:  
  All experiments are logged automaticallyâ€”including metrics, validation curves, and class-wise performance.

- **ğŸ“‚ Checkpoints**:  
  The best-performing expert checkpoint (by `val_s_score`) is saved to disk. Intermediate class weights are also preserved.

---

## ğŸ§  Supported Architectures

| Model Flag | Description |
|------------|-------------|
| `resnext` | ResNeXt-50 32x4d |
| `efficientnet` | EfficientNet-B4 |
| `vit-b` | Vision Transformer Base |
| `clip-l` | CLIP-pretrained ViT-Large |
| `vits16`, `vits8` | DINO ViT-Small (patch sizes 16/8) |
| `vitb16`, `vitb8` | DINO ViT-Base (patch sizes 16/8) |

ğŸ§° Add your own by extending `nets.py` with the `@register_model` decorator.

---

## ğŸ’¡ Citation

```bibtex
@misc{retinalboost2025,
  title={Retinal Disease Classification with Sequential Expert Training},
  author={Elham Soltani Kazemi},
  year={2025},
  url={https://github.com/es15326/Multi-Disease-Detection.git}
}
```

---

## ğŸ¤ Contact

Have questions, ideas, or feedback?  
ğŸ“¬ Reach out: [email@example.com](mailto:email@example.com)

---

## ğŸ† Performance

Evaluated on the [RFMiD](https://www.kaggle.com/datasets/rishitdagli/retinal-fundus-image-for-multi-disease-detection) dataset with 45 disease labels:

| Model             | F1 (Macro) | AUC (Macro) | Notes                      |
|------------------|------------|-------------|----------------------------|
| ViT (Single)     | 0.765      | 0.872       | No boosting or reweighting |
| Expert Chain (Ours) | **0.826**  | **0.912**    | 10 experts, ViT-Base        |

---

## ğŸ“Š Dataset

This project uses the publicly available **[RFMiD dataset](https://www.kaggle.com/datasets/rishitdagli/retinal-fundus-image-for-multi-disease-detection)** for training and evaluation. It contains:

- 3200 high-resolution retinal fundus images
- 45 disease labels (multi-label format)
- Imbalanced distribution across rare and common diseases

The boosting strategy helps adaptively improve under-represented diseases.

---

<p align="center">
  <img src="banner.png" width="85%" alt="Boosting Architecture Overview">
</p>
